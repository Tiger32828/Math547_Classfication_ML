# -*- coding: utf-8 -*-
"""547_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uf2kExIXYlpvHo-usS0P7karqZYjcgfz
"""

# 547 Final Project
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib as mlt
import seaborn as sns

import os

# Load Data
df_origin = pd.read_csv('secondary_data.csv', delimiter= ';')
df = df_origin.copy()
df.sample(10)

# Create dummy variable (since categorical variables)
# Splite the featuers and target in two variables.
X = df.drop('class', axis=1)
y = df['class']

# For the sake of visualization, return it by hot-encoding.
y = y.map({'p': 'Posionous', 'e': 'Edible'})

# Get dummy variables for each column in X.
X_dummy = pd.get_dummies(X, drop_first=True)
X_dummy.sample(5)

print('Number of columns generated from dummy variables =', len(X_dummy.columns.values),'Columns')

# dimensional reduction -- pca
# step1: standardize data
from sklearn.preprocessing import StandardScaler
X_std = StandardScaler().fit_transform(X_dummy)
X_std

# step2: PCA
from sklearn.decomposition import PCA
pca = PCA(n_components=5)
X_pca = pca.fit_transform(X_std)

print('Original Number of features =',  X_dummy.shape[1] ,'columns')
print('Reduced Number of features =', X_pca.shape[1], 'columns')

round(sum(list(pca.explained_variance_ratio_))*100, 2) # variance explain for pca, it is pretty low

# pca result is not good, try to calculate how much variable needed to capture 93%
pca = PCA(n_components=0.93, whiten=True)

# Conduct PCA
X_pca = pca.fit_transform(X_std)
# Show the result
print('Original number of features:', X_dummy.shape[1])
print('Reduced number of features:', X_pca.shape[1])

# we need 73 variable to capture 93%, which is a lot. We might try other dimensional reduction method
# use t-SNE,
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=0, perplexity=50, n_iter=5000)
X_tsne = tsne.fit_transform(X_std)

# Create a dataframe to plot
tsne_data = np.vstack((X_tsne.T, y)).T
df_tsne = pd.DataFrame(data=tsne_data, columns=['Dim_1', 'Dim_2', 'Class'])

# Plot the dataframe
sns.FacetGrid(data=df_tsne, hue='Class', size=6)\
   .map(plt.scatter, 'Dim_1', 'Dim_2')\
   .add_legend()
plt.title('With Perplexity = 70, n_iter = 1000');

# Train the model
# label encode the class column
y = y.map({'Posionous':1, 'Edible':0})
y.value_counts()

# methods import
from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble
MLA = [
    # Ensemble Methods
    ensemble.RandomForestClassifier(),


    # GLM
    linear_model.LogisticRegressionCV(),

    # Naives Bayes
    naive_bayes.BernoulliNB(),

    # Nearest Neighbor
    neighbors.KNeighborsClassifier(),

    # SVM
    svm.SVC(probability=True),

    # Trees
    tree.DecisionTreeClassifier()
]

# data split
# Splite dataset in cross-validation with this splitter class:
# NOTE: this is an alternative to train_test_split
from sklearn.model_selection import ShuffleSplit, cross_validate

# Run the model 10x with 60/30 split intentionally leaving out 10% of the data
cv_split = ShuffleSplit(n_splits=10, test_size=.3, train_size=.6, random_state=0)

# Create table to compare MLA metrices
MLA_columns = ['MLA Name', 'MLA Parameters','MLA Test Accuracy Mean',
               'MLA Test Accuracy 3*STD', 'MLA Time']
MLA_compare = pd.DataFrame(columns=MLA_columns)

# Create table to compare MLA Predictions
MLA_predict = {}

# build
for row_idx, alg in enumerate(MLA):
    # Set name and parameters
    MLA_name = alg.__class__.__name__
    MLA_compare.loc[row_idx, 'MLA Name'] = MLA_name
    MLA_compare.loc[row_idx, 'MLA Parameters'] = str(alg.get_params())

    # Score model with cross validation
    cv_results = cross_validate(alg, X_pca, y.astype('int'), cv=cv_split)
    MLA_compare.loc[row_idx, 'MLA Time'] = cv_results['fit_time'].mean()
#     MLA_compare.loc[row_idx, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()
    MLA_compare.loc[row_idx, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()

    # If this is a non-bias random sample, then +/-3 standard deviations from the mean
    # should statistically capture 99.7% of the subsets.
    MLA_compare.loc[row_idx, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3

    # Save MLA predictions
    alg.fit(X_pca, y.astype('int'))
    MLA_predict[MLA_name] = alg.predict(X_pca)

# Print and Sort table.
MLA_compare.sort_values(by=['MLA Test Accuracy Mean'], ascending=False,
                       inplace=True)
MLA_compare

plt.figure(figsize=(16, 8))
base_color = sns.color_palette()[1]
sns.barplot(x='MLA Test Accuracy Mean', y='MLA Name', data=MLA_compare, color=base_color)
plt.title('Machine Learning Algorithm Accuracy Score')
plt.xlabel('Accuracy Score (%)')
plt.ylabel('Algorithm Name')
mlt.rc('ytick', labelsize=14)